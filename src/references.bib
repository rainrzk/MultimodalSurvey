@inproceedings{radford2021clip,
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year      = {2021},
  pages     = {8748--8763},
  publisher = {PMLR}
}
@inproceedings{dosovitskiy2021vit,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021}
}
@inproceedings{kim2021vilt,
  title     = {ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author    = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year      = {2021},
  publisher = {PMLR}
}
@article{liu2023llava15,
  title   = {Improved Baselines with Visual Instruction Tuning},
  author  = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal = {arXiv preprint arXiv:2310.03744},
  year    = {2023}
}
@article{liu2023llava,
  title   = {Visual Instruction Tuning},
  author  = {Liu, Haotian and Lin, Chunyuan and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal = {arXiv preprint arXiv:2304.08485},
  year    = {2023}
}
@article{ye2023mplug2,
  title   = {mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video},
  author  = {Ye, Shengze and Qiao, Yu and Zhao, Yuxin and Deng, Yang and Cao, Peng and Chen, Jun and Liu, Yi and Ke, Wei and Yang, Xiaofeng and others},
  journal = {arXiv preprint arXiv:2302.00402},
  year    = {2023}
}
@inproceedings{ye2022mplug,
  title     = {mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-Connections},
  author    = {Ye, Shengze and Cao, Peng and Chen, Jun and Liu, Yi and Qiao, Yu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2022}
}
@inproceedings{wei2022cot,
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022}
}
@article{yao2023tot,
  title   = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author  = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Xu, Yuanfang and Dai, Bo and Shen, Yelong and Chen, Danqi},
  journal = {arXiv preprint arXiv:2305.10601},
  year    = {2023}
}
@inproceedings{besta2024got,
  title     = {Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
  author    = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Micha{\l} and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  volume    = {38},
  number    = {16},
  pages     = {17682--17690},
  year      = {2024},
  publisher = {AAAI Press},
  doi       = {10.1609/aaai.v38i16.29720}
}
@inproceedings{yao2023react,
  title     = {ReAct: Synergizing Reasoning and Acting in Language Models},
  author    = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2023}
}
@inproceedings{shinn2023reflexion,
  title     = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  author    = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS) 2023},
  year      = {2023}
}
@inproceedings{devlin2019bert,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  year      = {2019}
}
@inproceedings{lu2019vilbert,
  title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author    = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}
@article{openai2023gpt4,
  title   = {GPT-4 Technical Report},
  author  = {{OpenAI}},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}
@article{zhang2023mmcot,
  title   = {Multimodal Chain-of-Thought Reasoning in Language Models},
  author  = {Zhang, Yi and Li, Hao and Liu, Xinyang and Zhang, Peng and Cao, Yixin and Liu, Qun},
  journal = {arXiv preprint arXiv:2302.00923},
  year    = {2023}
}
@article{edge2024graphrag,
  title   = {From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
  author  = {Edge, D. and others},
  journal = {arXiv preprint arXiv:2404.16130},
  year    = {2024}
}
@article{wu2023autogen,
  title   = {AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author  = {Wu, Qingyun and Bansal, Gagan and Zhang, Jeffrey and Wu, Yiran and Li, Bei and Santhanam, Suresh and Zhang, Zixuan and Wang, Junjie and Gonzalez, Oscar and Peng, Yanjun and others},
  journal = {arXiv preprint arXiv:2308.08155},
  year    = {2023}
}
@article{ye2023mplugowl,
  title   = {mPLUG-Owl: Modularization Empowers Large Language Models with Multimodal Reasoning},
  author  = {Ye, Shengze and Chen, Jun and Zhao, Yuxin and Deng, Yang and Cao, Peng and Liu, Yi and Qiao, Yu},
  journal = {arXiv preprint arXiv:2304.14178},
  year    = {2023}
}
@article{hendricks2021decoupling,
  title   = {Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers},
  author  = {Hendricks, Lisa Anne and Mellor, John and Schneider, Rosalia and Alayrac, Jean-Baptiste and Nematzadeh, Aida},
  journal = {Transactions of the Association for Computational Linguistics (TACL)},
  volume  = {9},
  pages   = {570--585},
  year    = {2021}
}
